<!DOCTYPE html>
<html lang="en">
    <link rel="stylesheet" href="/assets/css/main.css">
    <body>

    <img alt="fallbrook"
src="/assets/img/logo.jpg"/>

    <nav class="footer">
  <a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
  <!-- <a class="editor-link btn" href="cloudcannon:collections/_data/navigation.yml" class="btn"><strong>&#9998;</strong> Edit navigation</a> -->
  
    
    

    

    <a href="/#main
" class=" highlight" >Home  |</a>
  
    
    

    

    <a href="/services
" class=" highlight" >Services  |</a>
  
    
    

    

    <a href="/posts
" class="" >Blog  |</a>
  
    
    

    

    <a href="/contact
" class="" >Contact</a>
  
</nav>


      <section id="about">
        <!-- header style="background-image: url('/white1.jpg')"-->

  <article class="feature-image">
<div class="postnav1">
    
      
        [ <a class="highlight" href="/2018/09/04/factor-models-and-risk-exposure.html"> Factor Models & Risk Exposure </a> &nbsp;] <&nbsp;&nbsp;<&nbsp;&nbsp;
    
    
       &nbsp;&nbsp;>&nbsp;&nbsp;> [ <a href="/2019/07/07/aws-data-pipelines.html">AWS Data Pipelines</a> ]
    
 </div>
  <div>

    <header style="background-image: url('/assets/img/post-headers/white1.jpg')">
       <h2 class="title">Dreaming Deeply with Neural Networks</h2>

       

       <p class="meta">
          May 29, 2019
           - R. Dubnick
       </p>
    </header>

    <section class="post-content"><h3 id="deep-dream-algorithm-and-image-over-processing">Deep Dream algorithm and image over-processing</h3>

<p><img src="/assets/img/blog-cnn/input.jpg" alt="input.jpg" />
<img src="/assets/img/blog-cnn/input_color1.jpeg" alt="input0.jpeg" />
<img src="/assets/img/blog-cnn/dd-in11.jpeg" alt="dd-in11.jpeg" /></p>

<p>Neural network models are not new, the first ones arrived in the 1950’s.  These Neural Networks are a subfield of machine learning inspired by the structure and function of the brain.  A neural network makes classifications or predictions based on its ability to discover patterns in data.  We might only find simple patterns with one layer however with more than one more complex patterns of patterns could be recognized.</p>

<p>Growth of rapid linear algebra hardware &amp; software stacks, cloud resources, proliferation of repeatable and shared research have all helped accelerate the evolution of new neural network models.</p>

<p><a href="https://en.wikipedia.org/wiki/DeepDream" target="_blank">Deep Dream</a> is a computer vision program created by Google engineer Alex Mordvintsev which uses a convolutional neural network to find and enhance patterns in images.  The resulting algorithmic pareidolia produces facinating images having a dream-like, kind of hallucinogenic appearance.</p>

<p>Depending on how models are developed this can rely on every area of the natural sciences and has applications in architecture, digital fabrication, art, engineering, biomedicine, etc.  In general, convolutional neural networks (CNNs) reflect a connectivity pattern manifest among neurons.  Multi-layer neural networks trained with a backpropagation algorithm is a gradient-based learning technique.  This approach resembles how the visual cortex functions in biology.  Inspired by nature, CNNs exploit local correlations and have significantly progressed fields of speech and image classification.</p>

<p>This generative progression is a kind of <a href="https://en.wikipedia.org/wiki/Digital_morphogenesis" target="_blank">digital morphogenesis</a> allowing us to synthesize visual textures and recognize patterns.  This technique relfects a kind of digital confirmation bias existing wthin the computational model where emergent patterns depend on the variety of subjects the model was trained with and the layers of the model we filter.</p>

<p>These very useful tools are built upon well-known mathematical methods, we actually understand surprisingly little of why certain models work and others don’t. So let’s take a look at some simple techniques for peeking inside these networks.  Computer Vision, Facial Recognition, and Speech Recognition, Self-driving cars are types of applications abound.</p>

<p>Artificial neural networks are trained via millions of training examples and gradually adjusting the network parameters until it gives the desired classification.  A network typically consists of 10-30 stacked layers of artificial neurons. Any given image is fed into the input layer, which then talks to the next layer, until eventually the “output” layer is reached. The network’s “answer” comes from its final output layer.</p>

<p>It remains a challenge to understand exactly what is going on at each layer. We know that after training, each layer progressively extracts higher and higher-level features of the image, until the final layer essentially makes a decision on what the image shows. For example, the first layer maybe looks for edges or corners. Intermediate layers interpret the basic features to look for overall shapes or components, like a door or a leaf. The final few layers assemble those into complete interpretations—these neurons activate in response to very complex things such as entire buildings or trees.</p>

<h3 id="producing-dreams">Producing Dreams</h3>

<p><img src="/assets/img/blog-cnn/intro0.jpg" alt="intro0.jpeg" />
<img src="/assets/img/blog-cnn/intro1.jpg" alt="intro1.jpeg" /></p>

<p>Looking at Deep Dream, we have a computer vision program created by Google engineer Alex Mordvintsev that takes advantage of convolutional neural networks to recgonize and augment patterns in images, creating some amazing Dream-like hallucinogenic appearances in the deliberately over-processed images.  The idea is to interpret a vague stimulus as something known to the observer, such as seeing shapes in clouds, seeing faces in inanimate objects or abstract patterns, or hearing hidden messages in music.  Making the “dream” images is a gradient ascent process which seeks maximize the L2 norm of activations of a particular DNN layer. Here are a few simple tricks that we found useful for getting good images:</p>

<p>offset image by a random jitter
normalize the magnitude of gradient ascent steps
apply ascent across multiple scales (octaves)
First we implement a basic gradient ascent step function, applying the first two tricks. Next we implement an ascent through different scales. We call these scales “octaves”.  Now we are ready to let the neural network to reveal its dreams! Let’s take a our site logo image as a starting point:</p>

<p>Running the next code cell starts the detail generation process. You may see how new patterns start to form, iteration by iteration, octave by octave.
The complexity of the details generated depends on which layer’s activations we try to maximize. Higher layers produce complex features, while lower ones enhance edges and textures, giving the image an impressionist feeling.</p>

<p><img src="/assets/img/blog-cnn/input.jpg" alt="input0.jpeg" />
<img src="/assets/img/blog-cnn/input0.jpeg" alt="input0.jpeg" />
<img src="/assets/img/blog-cnn/input_color1.jpeg" alt="input0.jpeg" /></p>

<p>What if we feed the deepdream function its own output, after applying a little zoom to it? It turns out that this leads to an endless stream of impressions of the things that the network saw during training. Some patterns fire more often than others, suggestive of basins of attraction.</p>

<p>We will start the process from the site logo image as above, but after some iteration the original image becomes irrelevant; even random noise can be used as the starting point.</p>

<p><img src="/assets/img/blog-cnn/input1.jpeg" alt="input0.jpeg" />
<img src="/assets/img/blog-cnn/input6a.jpeg" alt="input0.jpeg" />
<img src="/assets/img/blog-cnn/input16xxx.jpeg" alt="input0.jpeg" /></p>

<p>So here’s one surprise: neural networks that were trained to discriminate between different kinds of images have quite a bit of the information needed to
generate
images too. Check out some more examples across different classes:</p>

<p>Why is this important? Well, we train networks by simply showing them many examples of what we want them to learn, hoping they extract the essence of the matter at hand (e.g., a fork needs a handle and 2-4 tines), and learn to ignore what doesn’t matter (a fork can be any shape, size, color or orientation). But how do you check that the network has correctly learned the right features? It can help to visualize the network’s representation of a fork.</p>

<p>Indeed, in some cases, this reveals that the neural net isn’t quite looking for the thing we thought it was. For example, here’s what one neural net we designed thought dumbbells looked like:</p>

<p>There are dumbbells in there alright, but it seems no picture of a dumbbell is complete without a muscular weightlifter there to lift them. In this case, the network failed to completely distill the essence of a dumbbell. Maybe it’s never been shown a dumbbell without an arm holding it. Visualization can help us correct these kinds of training mishaps.</p>

<p>Instead of exactly prescribing which feature we want the network to amplify, we can also let the network make that decision. In this case we simply feed the network an arbitrary image or photo and let the network analyze the picture. We then pick a layer and ask the network to enhance whatever it detected. Each layer of the network deals with features at a different level of abstraction, so the complexity of features we generate depends on which layer we choose to enhance. For example, lower layers tend to produce strokes or simple ornament-like patterns, because those layers are sensitive to basic features such as edges and their orientations.</p>

<p>Left: Original photo by Zachi Evenor. Right: processed by Günther Noack, Software Engineer</p>

<p>Left: Original painting by Georges Seurat. Right: processed images by Matthew McNaughton, Software Engineer
If we choose higher-level layers, which identify more sophisticated features in images, complex features or even whole objects tend to emerge. Again, we just start with an existing image and give it to our neural net. We ask the network: “Whatever you see there, I want more of it!” This creates a feedback loop: if a cloud looks a little bit like a bird, the network will make it look more like a bird. This in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere.</p>

<p>The results are intriguing—even a relatively simple neural network can be used to over-interpret an image, just like as children we enjoyed watching clouds and interpreting the random shapes. This network was trained mostly on images of animals, so naturally it tends to interpret shapes as animals. But because the data is stored at such a high abstraction, the results are an interesting remix of these learned features.</p>

<p>Of course, we can do more than cloud watching with this technique. We can apply it to any kind of image. The results vary quite a bit with the kind of image, because the features that are entered bias the network towards certain interpretations. For example, horizon lines tend to get filled with towers and pagodas. Rocks and trees turn into buildings. Birds and insects appear in images of leaves.</p>

<p>The original image influences what kind of objects form in the processed image.
This technique gives us a qualitative sense of the level of abstraction that a particular layer has achieved in its understanding of images. We call this technique “Inceptionism” in reference to the neural net architecture used. See our Inceptionism gallery for more pairs of images and their processed results, plus some cool video animations.</p>

<p>We must go deeper: Iterations</p>

<p>If we apply the algorithm iteratively on its own outputs and apply some zooming after each iteration, we get an endless stream of new impressions, exploring the set of things the network knows about. We can even start this process from a random-noise image, so that the result becomes purely the result of the neural network, as seen in the following images:</p>

<p>Neural net “dreams”— generated purely from random noise, using a network trained on places by MIT Computer Science and AI Laboratory. See our Inceptionism gallery for hi-res versions of the images above and more (Images marked “Places205-GoogLeNet” were made using this network).</p>

<p>The techniques presented here help us understand and visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training. It also makes us wonder whether neural networks could become a tool for artists—a new way to remix visual concepts—or perhaps even shed a little light on the roots of the creative process in general.</p>
</section>
    <!-- {-% include tags_list.html tags=page.tags tag_count=page.tags.size %-} -->
  </div>
 </article>
<div class="postnav1">
    
      
        [ <a class="highlight" href="/2018/09/04/factor-models-and-risk-exposure.html"> Factor Models & Risk Exposure </a> &nbsp;] <&nbsp;&nbsp;<&nbsp;&nbsp;
    
    
       &nbsp;&nbsp;>&nbsp;&nbsp;> [ <a href="/2019/07/07/aws-data-pipelines.html">AWS Data Pipelines</a> ]
    
 </div>

   <div class="comments">
      
        <div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'fallbrookr';
        /* ensure that pages with query string get the same discussion */
            var url_parts = window.location.href.split("?");
            var disqus_url = url_parts[0];
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
      
   </div>

   <!-- Post navigation if site.theme_settings.post_navigation -->

   <!--  endif -->

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


      </section>

    
<div id="text-14" class="widget widget_text">
<!--
	<h4>Let&#8217;s Talk!</h4>
	<div style="float: left; padding: 0 10px 0 0;"><i class="icon-normal fa fa-phone accent-color"></i></div>
	<p>+1 (800) 395-9349</p>
	</div>
-->
<footer class="footer">
	<i class="icon-normal fa fa-envelope accent-color"></i>
	<a href="mailto:info@fallbrookresearch.com" target="_blank" rel="noopener">info@fallbrookresearch.com </a>
	<p>&copy; Fallbrook Research & Analytics, LLC 2019 </p>
    <!-- <p>via Jekyll <a href="https://github.com/d00d">RRD</a></p> -->
</footer>

<script src="particles.js"></script>
<script src="js/app.js"></script>
<script src="/assets/js/sweet-scroll.min.js"></script>
<script src="/assets/js/main.js"></script>

<!-- Google Analytics Include -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-117521095-1', 'auto');
ga('send', 'pageview');
</script>





  </body>
</html>
